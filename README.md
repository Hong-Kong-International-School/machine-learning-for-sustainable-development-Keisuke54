# PhysiGPT

## Background
In recent days, many large language models (LLMs) are being published, but a lot of forefront models are with subscription. There is a growing concern that the financial accesibility to those learning material will bring a huge inequality in education. Hence, this project aims to achieve a model that specializes in AP Physics C to support people's learning. 

### SDG's goal **Quality Educaiton**

## System details
The system included in this repository is speech to text (voice recognition), the main language model about AP Physics C, and text to speech to achieve Q&A expereince for physics concepts. 

* ### What it does
You speak out your AP Physics C question to the modle. The model understands the speech and responds to your question by speeaking out. 

* ### How it works
The model recognizes speech via OpenAI's Whisper speech recognition model and outputs text 
The converted text is passed through the langauge model and get the response
The response is then converted into speech via Google's Text-to-Speech model 

* ### How to install it + How to use it
Given the limitation in time to work, there is no app to downlod this model yet (real time Q&A app), but you can doanload the PihsiGPT file in this repository and save your recording in mp3 file and to get output speech. 

## Model Details
The main large language model is built with karas sequential model consisting of Embedding, two LSTM, and Dense layers. Embedding layer is responsible for 

The idea of this sequantial model as inspired by the [work by Franz Geffke](https://f-a.nz/dev/develop-your-own-llm-like-chatgpt-with-tensorflow-and-keras/).

The input sequence is a collection of numbers (np.arrary) that corresponds to each character from the The ouput data is a collection of numbers that corresponds to the next text to each collection of input data, as shown below.
Each text is converted into sequence using tokenizer. 

> "Apple" --> input = [Ap, pp, pl], output = [p, l, e]
> input_sequence = [[2, 1], [1, 1], [1, 3]], output_sequence = [1, 3, 4]

## Data sources used for training
As shown in 'DataGeneration.py' file, the training data was generated by first amplifying the initial prompts with a certain number times by passing it to Cohere using API, and then answering all the  questions by passing them to Cohere using API again. After the conversations were generated, I roughly speculated and removed the conversations that seem to lower the quality of the model significantly. 

## Design process + Stages of development
The initial idea was to train the language model with a single conversation and train with new conversations on top of the previous model. However, all the trials seemed to get trained only for the last datum/conversation, or the training time was not logical for the accuray achieved, this method was not used. Hence I merged the sequences for all the data/conversations to one arrary to train at a single time. Finally, setting random seed when training the model to make it reproducable was not the right iddea. It will make the model respond the same thing no matter what the prompts is. 

## Model performance
The language model trained specific for AP Physics C returns logical sentences in most trials, but it can be a broken repitiion of words. Also, although the sentences are compelte, the content of the responses are far away from the exact answer. For example, asking toque question which is from mechanics will return magnetic flux which is from electricity and magnetism. This is specualted to be mainly beacause of the lack of data, but it is not always the case. When the model was trained with more dataset, the training time was signifiacntly longerbut the accuracy did not improve. The ouput began with more reasonable starting words but the following characters/words became worse, such sa repeating the same words infinitely. 

## Future work
Exploring the tradeoff between the logical starting wordsa nd rhe accuracy of the model will be interesting. 
Explring differetn model architecture for better text recognition. 
Making an app for PhisiGPT.py model will enable practical usage of this system. 