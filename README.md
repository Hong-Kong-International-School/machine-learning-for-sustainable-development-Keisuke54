# PhysiGPT

## Background
In recent days, many large language models (LLMs) are being published, but a lot of forefront models cost money. This has amd will continue to cause a huge inequality in educaiton due to accessibility. Hence, this project aims to achieve a model that supports peope's leanring through machien learning model, specificaly for AP Physics C. 
**Quality Educaiton**

## System details
The system included in this repository is the main language model about AP Physics C, speech to text (voice recognition), and text to speech to achieve real time Q&A expereince for physics concepts. 

### What it does
You speek to the model, they'll understand the speech, and will respond to your Physics question by speeaking out 

### How it works
The model recognizes speech via OpenAI's Whisper speech recognition model and outputs text 
The converted text is passed through the LLM and get the response
The response is then converted into speech via Google's Text-to-Speech model 

### How to install it + How to use it
Given the limitation in time to work, there is no app to downlod this model yet (real time Q&A app), but you can doanload the PihsiGPT file in this repository and save your recording in mp3 file and to get output speech. 

## Model Details
The main large language model is built with karas sequential model consisting of Embedding, two LSTM, and Dense layers. Embedding layer is responsible for 

The idea of this sequantial model as inspired by the [work by Franz Geffke](https://f-a.nz/dev/develop-your-own-llm-like-chatgpt-with-tensorflow-and-keras/)/.

The input sequence is a collection of numbers (np.arrary) that corresponds to each character from the The ouput data is a collection of numbers that corresponds to the next text to each collection of input data, as shown below.
Each text is converted into sequence using tokenizer. 

> "Apple" --> input = [Ap, pp, pl], output = [p, l, e]
> input_sequence = [[2, 1], [1, 1], [1, 3]], output_sequence = [1, 3, 4]

## Data sources used for training
As shown in 'DataGeneration.py' file, the training data was generated by first amplifying the initial prompts with a certain number times by passing it to Cohere using API, and then answering all the  questions by passing them to Cohere using API again. After the conversations were generated, I roughly speculated and removed the conversations that seem to lower the quality of the model significantly. 

## Model performance
The language model trained specific for AP Physics C 

## Design process

## Stages of development
The initial idea was to 
However, all the trials seemed to get trained only for the last datum/conversation, or the training time was out of logical amount for the model's accuracy, this method was not -. Hence I merged the sequences for all the data/conversations to one arrary to train at a single time. 

## Future work
When the model was trained with more dataset, the training time was signifiacntly longerbut the accuracy did not improve. The ouput began with more reasonable starting words but the following characters/words became worse, such sa repeating the same words infinitely. Exploring the tradeoff between the logical starting wordsa nd rhe accuracy of the model will be interesting. 